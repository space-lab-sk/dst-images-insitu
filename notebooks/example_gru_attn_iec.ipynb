{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../experiments/src\"))\n",
    "\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from dtw_measure import dtw_measure\n",
    "\n",
    "from utils import set_seed, get_torch_device, load_config, \\\n",
    "    count_parameters, apply_glorot_xavier, inspect_gradient_norms\n",
    "\n",
    "from preprocessing import get_k_fold, load_data, get_torch_data\n",
    "from preprocessing import StandardScaler\n",
    "\n",
    "from postprocessing import save_gradient_norms_plot, save_predictions_and_true_values_plot, \\\n",
    "    save_predictions_detail_plot, save_scatter_predictions_and_true_values, \\\n",
    "    get_dst_rmse, get_detail_properties, get_dtw_measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        # ----split coronagraph data (first four columns) and other data\n",
    "        quadrands_inputs = inputs[:, :, :4].to(device)\n",
    "        inputs = inputs[:, :, 4:].to(device)\n",
    "        #---------------\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, quadrands_inputs)\n",
    "        targets = targets.squeeze(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    mean_loss = total_loss / len(train_loader)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def validate_model(model, val_test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_test_loader:\n",
    "            quadrands_inputs = inputs[:, :, :4].to(device)\n",
    "            inputs = inputs[:, :, 4:].to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, quadrands_inputs)\n",
    "            all_outputs.append(outputs.squeeze(-1))\n",
    "            targets = targets.squeeze(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    mean_loss = total_loss / len(val_test_loader)\n",
    "    all_outputs = torch.cat(all_outputs, dim=0)\n",
    "    return mean_loss, all_outputs\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Scaled Dot-Product Attention\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(QKVAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        query: [batch_size, 1, hidden_size]  (previous hidden state)\n",
    "        key: [batch_size, seq_len, hidden_size] (GRU outputs)\n",
    "        value: [batch_size, seq_len, hidden_size] (GRU outputs)\n",
    "        \"\"\"\n",
    "        attn_output, attn_weights = self.mha(query, key, value)\n",
    "        attn_output = attn_output.squeeze(1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "\n",
    "class GRUWithQKVAttention(nn.Module):\n",
    "    \"\"\"GRU layer followed by QKV Self-Attention\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
    "        super(GRUWithQKVAttention, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.0)\n",
    "        self.attention = QKVAttention(hidden_size, num_heads)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        output, _ = self.gru(x, h)  # [batch_size, seq_len, hidden_size]\n",
    "        context, attention_weights = self.attention(output[:, -1:, :], output, output)\n",
    "        return context, attention_weights\n",
    "    \n",
    "\n",
    "class GRUModelWithAttention(nn.Module):\n",
    "    \"\"\"Complete Model with GRU + Multi-Head QKV Attention\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, hidden_size2, output_size, num_gru_layers, dropout, num_heads):\n",
    "        super(GRUModelWithAttention, self).__init__()\n",
    "        \n",
    "        self.num_gru_layers = num_gru_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru1 = nn.GRU(input_size, hidden_size, num_gru_layers, batch_first=True, bidirectional=False, dropout=0.0)\n",
    "        \n",
    "        # found that mapping coronagraph visual features (Q-data) to 16-dim works best\n",
    "        self.linear1 = nn.Linear(4, 16)\n",
    "        \n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.gru2 = GRUWithQKVAttention(16, self.hidden_size2, num_gru_layers, num_heads)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size + hidden_size2, output_size)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, x_q):\n",
    "        h0 = torch.zeros(self.num_gru_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out1, _ = self.gru1(x, h0)\n",
    "\n",
    "        x_q = self.gelu(self.linear1(x_q))\n",
    "\n",
    "        h02 = torch.zeros(self.num_gru_layers, x_q.size(0), self.hidden_size2).to(x.device)\n",
    "        context, weights = self.gru2(x_q, h02)\n",
    "\n",
    "        concated = torch.cat((out1[:, -1, :], context), 1)\n",
    "        out = self.fc_out(concated)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_features(X, feature_indices, bins):\n",
    "    X_binned = X.copy()\n",
    "    for idx in feature_indices:\n",
    "        X_binned[..., idx] = np.digitize(X[..., idx], bins=bins, right=False)\n",
    "    return X_binned\n",
    "\n",
    "\n",
    "def separate_features(data, binned_indices, scaled_indices):\n",
    "    binned_features = data[..., binned_indices]\n",
    "    scaled_features = data[..., scaled_indices]\n",
    "    return binned_features, scaled_features\n",
    "\n",
    "\n",
    "def combine_features(binned, scaled):\n",
    "    return np.concatenate([binned, scaled], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#PART 1: EXPERIMENT CONFIGURATION SETUP\n",
    "########################################\n",
    "\n",
    "config_file_name = \"conf_gru_attn_iec_1_1_1.yaml\"\n",
    "seed_input = 42\n",
    "device_input = \"cpu\"\n",
    "\n",
    "config = load_config(f\"configs/conf_gru_attn_iec/{config_file_name}\")\n",
    "EXPERIMENT_NAME = config[\"logging\"][\"experiment_name\"]\n",
    "# uncomment this line to save also seed you running\n",
    "#EXPERIMENT_NAME = EXPERIMENT_NAME + \"__\" + seed_input\n",
    "EXPERIMENT_NOTES = config[\"logging\"][\"notes\"]\n",
    "\n",
    "set_seed(seed_input)\n",
    "device = get_torch_device(device_input)\n",
    "\n",
    "BATCH_SIZE = config[\"training\"][\"batch_size\"]\n",
    "LEARNING_RATE = config[\"training\"][\"learning_rate\"]\n",
    "NUM_EPOCHS = config[\"training\"][\"num_epochs\"]\n",
    "WEIGHT_DECAY = config[\"training\"][\"weight_decay\"]\n",
    "AUGUMENTATION_RATE = config[\"training\"][\"augumentation_rate\"]\n",
    "\n",
    "INPUT_SIZE = config[\"model\"][\"input_size\"]\n",
    "HIDDEN_CHANNELS1 = config[\"model\"][\"hidden_channels1\"]\n",
    "HIDDEN_CHANNELS2 = config[\"model\"][\"hidden_channels2\"]\n",
    "OUTPUT_SIZE = config[\"model\"][\"output_size\"]\n",
    "NUM_GRU_LAYERS = config[\"model\"][\"num_gru_layers\"]\n",
    "DROPOUT = config[\"model\"][\"dropout\"]\n",
    "\n",
    "TIME_STEPS = config[\"data\"][\"time_steps\"]\n",
    "PREDICTION_WINDOW = config[\"data\"][\"prediction_window\"]\n",
    "K_FOLD = config[\"data\"][\"k_fold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#PART 2: PREPARING DATA\n",
    "###########################\n",
    "\n",
    "file_ids_train, file_ids_val, file_ids_test = get_k_fold(K_FOLD)\n",
    "\n",
    "train_X_unscaled, train_y_unscaled = load_data(file_ids_train, time_steps=TIME_STEPS, sliding_window=PREDICTION_WINDOW)\n",
    "val_X_unscaled, val_y_unscaled = load_data(file_ids_val, time_steps=TIME_STEPS, sliding_window=PREDICTION_WINDOW)\n",
    "test_X_unscaled, test_y_unscaled = load_data(file_ids_test, time_steps=TIME_STEPS, sliding_window=PREDICTION_WINDOW)\n",
    "\n",
    "#standard_scaler = StandardScaler(train_X_unscaled, train_y_unscaled)\n",
    "\n",
    "#--------conver coronagraph visual features (Q-DATA) from number to category -- categorization------------\n",
    "# - similar as savgol filter in preprocessing.load_data() categorization can be left out, but we figured out that it gives small performance boost, so we kept it here\n",
    "\n",
    "standard_scaler = StandardScaler(train_X_unscaled[..., 4:], train_y_unscaled)\n",
    "\n",
    "bin_edges = [0, 3.0, 5.0, 15.0, 30.0, 100.0]  \n",
    "features_to_bin = [0, 1, 2, 3]\n",
    "features_to_scale = range(4, train_X_unscaled.shape[-1])\n",
    "\n",
    "# separate binned q-data and scaled features\n",
    "train_binned, train_to_scale = separate_features(train_X_unscaled, range(4), features_to_scale)\n",
    "val_binned, val_to_scale = separate_features(val_X_unscaled, range(4), features_to_scale)\n",
    "test_binned, test_to_scale = separate_features(test_X_unscaled, range(4), features_to_scale)\n",
    "\n",
    "# standardize only the scaled features\n",
    "train_scaled = standard_scaler.standardize_X(train_to_scale)\n",
    "val_scaled = standard_scaler.standardize_X(val_to_scale)\n",
    "test_scaled = standard_scaler.standardize_X(test_to_scale)\n",
    "\n",
    "train_X_binned = bin_features(train_binned, features_to_bin, bin_edges) \n",
    "val_X_binned = bin_features(val_binned, features_to_bin, bin_edges) \n",
    "test_X_binned = bin_features(test_binned, features_to_bin, bin_edges) \n",
    "\n",
    "# combine bined q-data and standardized numeric insitu\n",
    "train_X = combine_features(train_X_binned, train_scaled)\n",
    "val_X = combine_features(val_X_binned, val_scaled)\n",
    "test_X = combine_features(test_X_binned, test_scaled)\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "#train_X = standard_scaler.standardize_X(train_X_unscaled)\n",
    "#val_X = standard_scaler.standardize_X(val_X_unscaled)\n",
    "#test_X =standard_scaler.standardize_X(test_X_unscaled)\n",
    "\n",
    "# standardize y\n",
    "train_y = standard_scaler.standardize_y(train_y_unscaled)\n",
    "val_y = standard_scaler.standardize_y(val_y_unscaled)\n",
    "test_y = standard_scaler.standardize_y(test_y_unscaled)\n",
    "\n",
    "train_X, train_y = get_torch_data(train_X, train_y)\n",
    "val_X, val_y = get_torch_data(val_X, val_y)\n",
    "test_X, test_y = get_torch_data(test_X, test_y)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_X, val_y)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#PART 3: DEEP LEARNING PART\n",
    "###############################################\n",
    "\n",
    "model = GRUModelWithAttention(input_size=INPUT_SIZE, hidden_size=HIDDEN_CHANNELS1, hidden_size2=HIDDEN_CHANNELS2, output_size=OUTPUT_SIZE, num_gru_layers=NUM_GRU_LAYERS, dropout=DROPOUT, num_heads=2)\n",
    "model.to(device)\n",
    "apply_glorot_xavier(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total parameters of the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#PART 4: TRAINING LOOP\n",
    "############################\n",
    "\n",
    "print(f\"--------------TRAINING LOOP--------------\")\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "gradient_norms = []\n",
    "\n",
    "best_val = 10000.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, _ = validate_model(model, val_loader, criterion, device)\n",
    "\n",
    "    losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    #==============grad norms============\n",
    "    total_norm = inspect_gradient_norms(model)\n",
    "    gradient_norms.append(total_norm)\n",
    "    #==========================\n",
    "\n",
    "    print(f'{epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"model with val loss {val_loss} saved...\")\n",
    "\n",
    "print('Training completed saving....')\n",
    "#torch.save(best_model_state, f'models/{EXPERIMENT_NAME}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#PART 4: MODEL EVALUATION\n",
    "############################\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "test_loss, test_predictions_standardized = validate_model(model, test_loader, criterion, device)\n",
    "test_predictions_standardized = test_predictions_standardized.cpu()\n",
    "test_predictions = (test_predictions_standardized * standard_scaler.y_std) + standard_scaler.y_mean\n",
    "print(test_predictions_standardized.shape)\n",
    "test_predictions = test_predictions.numpy().tolist()\n",
    "print(f\"avg. test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list = test_y_unscaled.tolist()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(y_true_list, label=\"True values\", linewidth=0.5, color=\"green\", marker='o', markersize=3)\n",
    "plt.plot(test_predictions, label=\"Prediction\", linewidth=0.5, color=\"orange\", marker='o', markersize=3)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Dst Value [nT]')\n",
    "plt.title(\"Detail of test prediction\")\n",
    "plt.grid(axis='x', alpha=0.5, linestyle=\"--\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
